{"title":"现代游戏引擎 - 高级AI（十七）","slug":"Games104/17_Gameplay Systems-Advanced Artificial Intelligence","date":"2023-04-20T12:13:50.000Z","updated":"2023-11-29T02:42:01.742Z","comments":true,"path":"api/articles/Games104/17_Gameplay Systems-Advanced Artificial Intelligence.json","realPath":null,"excerpt":null,"covers":["/images/article/Games104/17/Games104_17_01.png","/images/article/Games104/17/Games104_17_02.png","/images/article/Games104/17/Games104_17_03.png","/images/article/Games104/17/Games104_17_04.png","/images/article/Games104/17/Games104_17_05.png","/images/article/Games104/17/Games104_17_06.png","/images/article/Games104/17/Games104_17_07.png","/images/article/Games104/17/Games104_17_09.png","/images/article/Games104/17/Games104_17_11.png","/images/article/Games104/17/Games104_17_13.png","/images/article/Games104/17/Games104_17_20.png","/images/article/Games104/17/Games104_17_21.png","/images/article/Games104/17/Games104_17_22.png","/images/article/Games104/17/Games104_17_23.png","/images/article/Games104/17/Games104_17_24.png","/images/article/Games104/17/Games104_17_25.png","/images/article/Games104/17/Games104_17_27.png","/images/article/Games104/17/Games104_17_28.png","/images/article/Games104/17/Games104_17_29.png","/images/article/Games104/17/Games104_17_33.png","/images/article/Games104/17/Games104_17_35.png","/images/article/Games104/17/Games104_17_36.png","/images/article/Games104/17/Games104_17_38.png","/images/article/Games104/17/Games104_17_39.png","/images/article/Games104/17/Games104_17_40.png","/images/article/Games104/17/Games104_17_43.png","/images/article/Games104/17/Games104_17_44.png","/images/article/Games104/17/Games104_17_45.png","/images/article/Games104/17/Games104_17_46.png","/images/article/Games104/17/Games104_17_47.png","/images/article/Games104/17/Games104_17_49.png","/images/article/Games104/17/Games104_17_50.png","/images/article/Games104/17/Games104_17_52.png","/images/article/Games104/17/Games104_17_53.png","/images/article/Games104/17/Games104_17_55.png","/images/article/Games104/17/Games104_17_56.png","/images/article/Games104/17/Games104_17_57.png","/images/article/Games104/17/Games104_17_58.png","/images/article/Games104/17/Games104_17_62.png","/images/article/Games104/17/Games104_17_64.png","/images/article/Games104/17/Games104_17_70.png","/images/article/Games104/17/Games104_17_71.png","/images/article/Games104/17/Games104_17_72.png","/images/article/Games104/17/Games104_17_75.png","/images/article/Games104/17/Games104_17_76.png","/images/article/Games104/17/Games104_17_78.png","/images/article/Games104/17/Games104_17_84.png","/images/article/Games104/17/Games104_17_85.png","/images/article/Games104/17/Games104_17_87.png","/images/article/Games104/17/Games104_17_89.png"],"cover":"/images/background/Gameplay Systems-Advanced Artificial Intelligence.jpg","content":"<h2 id=\"高级AI大纲\"><a href=\"#高级AI大纲\" class=\"headerlink\" title=\"高级AI大纲\"></a>高级AI大纲</h2><p><img src=\"/images/article/Games104/17/Games104_17_01.png\"><span class=\"image-caption\">高级AI大纲</span></p>\n<h2 id=\"层次任务网络（Hierarchical-Tasks-Network）\"><a href=\"#层次任务网络（Hierarchical-Tasks-Network）\" class=\"headerlink\" title=\"层次任务网络（Hierarchical Tasks Network）\"></a>层次任务网络（Hierarchical Tasks Network）</h2><p><b>层次任务网络(hierarchical tasks network, HTN)</b>是经典的游戏AI技术，和上一节介绍过的行为树相比HTN可以更好地表达AI自身的意志和驱动力。<br><img src=\"/images/article/Games104/17/Games104_17_02.png\"><span class=\"image-caption\">概括</span></p>\n<p>HTN的思想是把总体目标分解成若干个步骤，其中每个步骤可以包含不同的选项。AI在执行时需要按照顺序完成每个步骤，并且根据自身的状态选择合适的行为。<br><img src=\"/images/article/Games104/17/Games104_17_03.png\"><span class=\"image-caption\">像人一样制定一个计划</span></p>\n<h3 id=\"分层任务网络框架（HTN-Framework）\"><a href=\"#分层任务网络框架（HTN-Framework）\" class=\"headerlink\" title=\"分层任务网络框架（HTN Framework）\"></a>分层任务网络框架（HTN Framework）</h3><p>HTN框架中包含两部分，<b>world state</b>和<b>sensor</b>两部分。其中world state是AI对于游戏世界的认知，而sensor则是AI从游戏世界获取信息的渠道。<br><img src=\"/images/article/Games104/17/Games104_17_04.png\"><span class=\"image-caption\">分层任务网络框架</span></p>\n<p>除此之外HTN还包括<b>domain</b>，<b>planner</b>以及<b>plan runner</b>来表示AI的规划以及执行规划的过程。<br><img src=\"/images/article/Games104/17/Games104_17_05.png\"><span class=\"image-caption\">分层任务网络框架2</span></p>\n<h3 id=\"分层网络任务类型（HTN-Task-Types）\"><a href=\"#分层网络任务类型（HTN-Task-Types）\" class=\"headerlink\" title=\"分层网络任务类型（HTN Task Types）\"></a>分层网络任务类型（HTN Task Types）</h3><p>在HTN中我们将任务分为两类，<b>primitive task</b>和<b>compound task</b>。<br><img src=\"/images/article/Games104/17/Games104_17_06.png\"><span class=\"image-caption\">分层网络任务类型</span></p>\n<p>primitive task一般表示一个具体的动作或行为。在HTN中每个primitive task需要包含precondition、action以及effects三个要素。<br><img src=\"/images/article/Games104/17/Games104_17_07.png\"><span class=\"image-caption\">原始任务</span><br><img src=\"/images/article/Games104/17/Games104_17_08.png\"><span class=\"image-caption\">原始任务2</span></p>\n<p>而compound task则包含不同的方法，我们把这些方法按照一定的优先级组织起来并且在执行时按照优先级高到低的顺序进行选择。每个方法还可以包含其它的primitive task或者compound task，当方法内所有的task都执行完毕则表示任务完成。<br><img src=\"/images/article/Games104/17/Games104_17_09.png\"><span class=\"image-caption\">复合任务</span><br><img src=\"/images/article/Games104/17/Games104_17_10.png\"><span class=\"image-caption\">复合任务2</span></p>\n<p>在此基础上就可以构造出整个HTN的domain，从而实现AI的行为逻辑。<br><img src=\"/images/article/Games104/17/Games104_17_11.png\"><span class=\"image-caption\">HTN域</span><br><img src=\"/images/article/Games104/17/Games104_17_12.png\"><span class=\"image-caption\">HTN域2</span></p>\n<h3 id=\"规划（Planning）\"><a href=\"#规划（Planning）\" class=\"headerlink\" title=\"规划（Planning）\"></a>规划（Planning）</h3><p>接下来就可以进行规划了，我们从root task出发不断进行展开逐步完成每个任务。<br><img src=\"/images/article/Games104/17/Games104_17_13.png\"><span class=\"image-caption\">规划</span><br><img src=\"/images/article/Games104/17/Games104_17_14.png\"><span class=\"image-caption\">规划2</span><br><img src=\"/images/article/Games104/17/Games104_17_15.png\"><span class=\"image-caption\">规划3</span><br><img src=\"/images/article/Games104/17/Games104_17_16.png\"><span class=\"image-caption\">规划4</span><br><img src=\"/images/article/Games104/17/Games104_17_17.png\"><span class=\"image-caption\">规划5</span><br><img src=\"/images/article/Games104/17/Games104_17_18.png\"><span class=\"image-caption\">规划6</span><br><img src=\"/images/article/Games104/17/Games104_17_19.png\"><span class=\"image-caption\">规划7</span></p>\n<h3 id=\"重新规划（Replan）\"><a href=\"#重新规划（Replan）\" class=\"headerlink\" title=\"重新规划（Replan）\"></a>重新规划（Replan）</h3><p>执行plan时需要注意有时任务会失败，这就需要我们重新进行规划，这一过程称为<b>replan</b>。<br><img src=\"/images/article/Games104/17/Games104_17_20.png\"><span class=\"image-caption\">运行规划</span></p>\n<p>当plan执行完毕或是发生失败，亦或是world state发生改变后就需要进行replan。<br><img src=\"/images/article/Games104/17/Games104_17_21.png\"><span class=\"image-caption\">重新规划</span></p>\n<p>总结一下HTN和BT非常相似，但它更加符合人的直觉也更易于设计师进行掌握。<br><img src=\"/images/article/Games104/17/Games104_17_22.png\"><span class=\"image-caption\">总结</span></p>\n<h2 id=\"目标导向的AI系统（Goal-Oriented-Action-Planning-GOAP）\"><a href=\"#目标导向的AI系统（Goal-Oriented-Action-Planning-GOAP）\" class=\"headerlink\" title=\"目标导向的AI系统（Goal-Oriented Action Planning - GOAP）\"></a>目标导向的AI系统（Goal-Oriented Action Planning - GOAP）</h2><p><b>goal-oriented action planning(GOAP)</b>是一种基于规划的AI技术，和前面介绍过的方法相比GOAP一般会更适合动态的环境。<br><img src=\"/images/article/Games104/17/Games104_17_23.png\"><span class=\"image-caption\">目标导向的AI系统</span></p>\n<h3 id=\"结构（Structure）\"><a href=\"#结构（Structure）\" class=\"headerlink\" title=\"结构（Structure）\"></a>结构（Structure）</h3><p>GOAP的整体结构与HTN非常相似，不过在GOAP中domain被替换为<b>goal set</b>和<b>action set</b>。<br><img src=\"/images/article/Games104/17/Games104_17_24.png\"><span class=\"image-caption\">结构</span></p>\n<p>goal set表示AI所有可以达成的目标。在GOAP中需要显式地定义可以实现的目标，这要求我们把目标使用相应的状态来进行表达。<br><img src=\"/images/article/Games104/17/Games104_17_25.png\"><span class=\"image-caption\">目标集合</span><br><img src=\"/images/article/Games104/17/Games104_17_26.png\"><span class=\"image-caption\">目标选择</span></p>\n<p>而action set则接近于primitive task的概念，它表示AI可以执行的行为。需要注意的是action set还包含<b>代价(cost)</b>的概念，它表示不同动作的”优劣”程度。在进行规划时我们希望AI尽可能做出代价小的决策。<br><img src=\"/images/article/Games104/17/Games104_17_27.png\"><span class=\"image-caption\">操作集</span></p>\n<h3 id=\"规划（Planning）-1\"><a href=\"#规划（Planning）-1\" class=\"headerlink\" title=\"规划（Planning）\"></a>规划（Planning）</h3><p>GOAP在进行规划时会从目标来倒推需要执行的动作，这一过程称为<b>反向规划(backward planning)</b>。<br><img src=\"/images/article/Games104/17/Games104_17_28.png\"><span class=\"image-caption\">像一个人一样反向规划</span></p>\n<p>在进行规划时首先需要根据优先级来选取一个目标，然后查询实现目标需要满足的状态。为了满足这些状态需求，我们需要从action set中选择一系列动作。需要注意的是很多动作也有自身的状态需求，因此我们在选择动作时也需要把这些需求添加到列表中。最后不断地添加动作和需求直到所有的状态需求都得到了满足，这样就完成了反向规划。<br><img src=\"/images/article/Games104/17/Games104_17_29.png\"><span class=\"image-caption\">规划</span><br><img src=\"/images/article/Games104/17/Games104_17_30.png\"><span class=\"image-caption\">规划2</span><br><img src=\"/images/article/Games104/17/Games104_17_31.png\"><span class=\"image-caption\">规划3</span><br><img src=\"/images/article/Games104/17/Games104_17_32.png\"><span class=\"image-caption\">规划4</span></p>\n<p>GOAP的难点在于如何从action set进行选择，我们要求状态需求都能够得到满足而且所添加动作的代价要尽可能小。显然这样的问题是一个<b>动态规划(dynamic programming)</b>问题，我们可以利用图这样的数据结构来进行求解。在构造图时把状态的组合作为图上的节点，不同节点之间的有向边表示可以执行的动作，边的权重则是动作的代价。这样整个规划问题就等价于在有向图上的最短路径问题。<br><img src=\"/images/article/Games104/17/Games104_17_33.png\"><span class=\"image-caption\">构建状态-行动成本图</span><br><img src=\"/images/article/Games104/17/Games104_17_34.png\"><span class=\"image-caption\">最低成本路径</span></p>\n<p>总结一下GOAP可以让AI的行为更加动态，而且可以有效地解耦AI的目标与行为；而GOAP的主要缺陷在于它会比较消耗计算资源，一般情况下GOAP需要的计算量会远高于BT和HTN。<br><img src=\"/images/article/Games104/17/Games104_17_35.png\"><span class=\"image-caption\">总结</span></p>\n<h2 id=\"蒙特卡洛树搜索（Monte-Carlo-Tree-Search）\"><a href=\"#蒙特卡洛树搜索（Monte-Carlo-Tree-Search）\" class=\"headerlink\" title=\"蒙特卡洛树搜索（Monte Carlo Tree Search）\"></a>蒙特卡洛树搜索（Monte Carlo Tree Search）</h2><p><b>蒙特卡洛树搜索(Monte Carlo tree search, MCTS)</b>也是经典的AI算法，实际上AlphaGo就是基于MCTS来实现的。简单来说，MCTS的思路是在进行决策时首先模拟大量可行的动作，然后从这些动作中选择最好的那个来执行。<br><img src=\"/images/article/Games104/17/Games104_17_36.png\"><span class=\"image-caption\">蒙特卡洛树搜索</span><br><img src=\"/images/article/Games104/17/Games104_17_37.png\"><span class=\"image-caption\">蒙特卡洛树搜索2</span></p>\n<p>MCTS的核心是<b>Monte Carlo方法(Monte Carlo method)</b>，它指出定积分可以通过随机采样的方法来进行估计。<br><img src=\"/images/article/Games104/17/Games104_17_38.png\"><span class=\"image-caption\">Monte Carlo方法</span></p>\n<p>以围棋为例，MCTS会根据当前棋盘上的状态来估计落子的位置。<br><img src=\"/images/article/Games104/17/Games104_17_39.png\"><span class=\"image-caption\">蒙特卡洛树搜索3</span></p>\n<p>从数学的角度来看，我们把棋盘上棋子的位置称为<b>状态(state)</b>，同时把落子的过程称为<b>行为(action)</b>。这样整个游戏可以建模为从初始节点出发的状态转移过程，而且所有可能的状态转移可以表示为一棵树。<br><img src=\"/images/article/Games104/17/Games104_17_40.png\"><span class=\"image-caption\">状态和行动</span><br><img src=\"/images/article/Games104/17/Games104_17_41.png\"><span class=\"image-caption\">状态转移</span><br><img src=\"/images/article/Games104/17/Games104_17_42.png\"><span class=\"image-caption\">状态空间</span></p>\n<p>显然构造出完整的树结构可能是非常困难的，不过实际上我们并不需要完整的树。在使用MCTS时，完成每一个行为后只需要重新以当前状态构造一棵新树即可。<br><img src=\"/images/article/Games104/17/Games104_17_43.png\"><span class=\"image-caption\">注意：重建每次移动重建状态空间</span></p>\n<h3 id=\"模拟（Simulation）\"><a href=\"#模拟（Simulation）\" class=\"headerlink\" title=\"模拟（Simulation）\"></a>模拟（Simulation）</h3><p><b>模拟(simulation)</b>是MCTS中的重要一环，这里的”模拟”是指AI利用当前的策略快速地完成整个游戏过程。<br><img src=\"/images/article/Games104/17/Games104_17_44.png\"><span class=\"image-caption\">模拟：快速地玩游戏</span></p>\n<h3 id=\"反向传播（Backpropagate）\"><a href=\"#反向传播（Backpropagate）\" class=\"headerlink\" title=\"反向传播（Backpropagate）\"></a>反向传播（Backpropagate）</h3><p>我们从同一节点出发进行不断的模拟就可以估计该节点的价值(胜率)。<br><img src=\"/images/article/Games104/17/Games104_17_45.png\"><span class=\"image-caption\">如何评估这些状态？</span></p>\n<p>然后把模拟的结果从下向上进行传播就可以更新整个决策序列上所有节点的价值。<br><img src=\"/images/article/Games104/17/Games104_17_46.png\"><span class=\"image-caption\">反向传播</span></p>\n<h3 id=\"迭代步骤（Iteration-Steps）\"><a href=\"#迭代步骤（Iteration-Steps）\" class=\"headerlink\" title=\"迭代步骤（Iteration Steps）\"></a>迭代步骤（Iteration Steps）</h3><p>这样我们就可以定义MCTS的迭代步骤如下：<br><img src=\"/images/article/Games104/17/Games104_17_47.png\"><span class=\"image-caption\">迭代步骤</span><br><img src=\"/images/article/Games104/17/Games104_17_48.png\"><span class=\"image-caption\">在“无限”状态空间中进行搜索</span></p>\n<h4 id=\"选择（Selection）\"><a href=\"#选择（Selection）\" class=\"headerlink\" title=\"选择（Selection）\"></a>选择（Selection）</h4><p>在对节点进行选择时，MCTS会优先选择可拓展的节点。<br><img src=\"/images/article/Games104/17/Games104_17_49.png\"><span class=\"image-caption\">选择-可扩展节点</span></p>\n<p>在进行拓展时往往还要权衡一些exploitation和exploration，因此我们可以把UCB可以作为一种拓展的准则。<br><img src=\"/images/article/Games104/17/Games104_17_50.png\"><span class=\"image-caption\">选择-开发与勘探</span><br><img src=\"/images/article/Games104/17/Games104_17_51.png\"><span class=\"image-caption\">UCB（上置信度区间）</span></p>\n<p>这样在进行选择时首先需要从根节点出发然后不断选择当前UCB最大的那个节点向下进行访问，当访问到一个没有拓展过的节点时选择该节点进行展开。<br><img src=\"/images/article/Games104/17/Games104_17_52.png\"><span class=\"image-caption\">选择</span></p>\n<h4 id=\"拓展（Expansion）\"><a href=\"#拓展（Expansion）\" class=\"headerlink\" title=\"拓展（Expansion）\"></a>拓展（Expansion）</h4><p>对节点进行展开时我们需要根据可执行的动作选择一组进行模拟，然后把模拟的结果自下而上进行传播。<br><img src=\"/images/article/Games104/17/Games104_17_53.png\"><span class=\"image-caption\">拓展</span><br><img src=\"/images/article/Games104/17/Games104_17_54.png\"><span class=\"image-caption\">模拟和回溯</span></p>\n<h4 id=\"最终条件（The-End-Condition）\"><a href=\"#最终条件（The-End-Condition）\" class=\"headerlink\" title=\"最终条件（The End Condition）\"></a>最终条件（The End Condition）</h4><p>当对树的探索达到一定程度后就可以终止拓展过程，此时我们就得到了树结构上每个节点的价值。<br><img src=\"/images/article/Games104/17/Games104_17_55.png\"><span class=\"image-caption\">最终条件</span></p>\n<p>然后只需要回到根节点选择一个最优的子节点进行执行即可。<br><img src=\"/images/article/Games104/17/Games104_17_56.png\"><span class=\"image-caption\">如何选择最好的移动？</span></p>\n<p>总结一下，MCTS是一种非常强大的决策算法而且很适合搜索空间巨大的决策问题；而它的主要缺陷在于它具有过大的计算复杂度，而且它的效果很大程度上依赖于状态和行为空间的设计。<br><img src=\"/images/article/Games104/17/Games104_17_57.png\"><span class=\"image-caption\">总结</span></p>\n<h2 id=\"机器学习基础（Machine-Learning-Basic）\"><a href=\"#机器学习基础（Machine-Learning-Basic）\" class=\"headerlink\" title=\"机器学习基础（Machine Learning Basic）\"></a>机器学习基础（Machine Learning Basic）</h2><h3 id=\"机器学习技术（ML-Types）\"><a href=\"#机器学习技术（ML-Types）\" class=\"headerlink\" title=\"机器学习技术（ML Types）\"></a>机器学习技术（ML Types）</h3><p>近几年在<b>机器学习(machine learning, ML)</b>技术的不断发展下有越来越多的游戏AI开始使用机器学习来进行实现。根据学习的方式，机器学习大致可以分为监督学习、无监督学习、半监督学习以及强化学习等几类。<br><img src=\"/images/article/Games104/17/Games104_17_58.png\"><span class=\"image-caption\">四种类型的机器学习</span><br><img src=\"/images/article/Games104/17/Games104_17_59.png\"><span class=\"image-caption\">监督学习</span><br><img src=\"/images/article/Games104/17/Games104_17_60.png\"><span class=\"image-caption\">无监督学习</span><br><img src=\"/images/article/Games104/17/Games104_17_61.png\"><span class=\"image-caption\">半监督学习</span></p>\n<p><b>强化学习(reinforcement learning, RL)</b>是游戏AI技术的基础。在强化学习中我们希望AI能够通过和环境的不断互动来学习到一个合理的策略。<br><img src=\"/images/article/Games104/17/Games104_17_62.png\"><span class=\"image-caption\">强化学习</span><br><img src=\"/images/article/Games104/17/Games104_17_63.png\"><span class=\"image-caption\">强化学习2</span></p>\n<h3 id=\"Markov决策过程（Markov-Decision-Process）\"><a href=\"#Markov决策过程（Markov-Decision-Process）\" class=\"headerlink\" title=\"Markov决策过程（Markov Decision Process）\"></a>Markov决策过程（Markov Decision Process）</h3><p>强化学习的理论基础是<b>Markov决策过程(Markov decision process, MDP)</b>。在MDP中智能体对环境的感知称为<b>状态(state)</b>，环境对于智能体的反馈称为<b>奖励(reward)</b>。MDP的目标是让智能体通过和环境不断的互动来学习到如何在不同的环境下进行决策，这样的一个决策函数称为<b>策略(policy)</b>。<br><img src=\"/images/article/Games104/17/Games104_17_64.png\"><span class=\"image-caption\">Markov决策过程-基本要素</span><br><img src=\"/images/article/Games104/17/Games104_17_65.png\"><span class=\"image-caption\">Markov决策过程-状态</span><br><img src=\"/images/article/Games104/17/Games104_17_66.png\"><span class=\"image-caption\">Markov决策过程-行动</span><br><img src=\"/images/article/Games104/17/Games104_17_67.png\"><span class=\"image-caption\">Markov决策过程-奖励</span><br><img src=\"/images/article/Games104/17/Games104_17_68.png\"><span class=\"image-caption\">MDP数学模型</span><br><img src=\"/images/article/Games104/17/Games104_17_69.png\"><span class=\"image-caption\">策略</span></p>\n<h2 id=\"构建高级游戏AI（Build-Advanced-Game-AI）\"><a href=\"#构建高级游戏AI（Build-Advanced-Game-AI）\" class=\"headerlink\" title=\"构建高级游戏AI（Build Advanced Game AI）\"></a>构建高级游戏AI（Build Advanced Game AI）</h2><p>尽管目前基于机器学习的游戏AI技术大多还处于试验阶段，但已经有一些很优秀的项目值得借鉴和学习，包括DeepMind的AlphaStar以及OpenAI的Five等。<br><img src=\"/images/article/Games104/17/Games104_17_70.png\"><span class=\"image-caption\">为什么游戏人工智能需要机器学习</span></p>\n<p>这些基于<b>深度强化学习(deep reinforcement learning, DRL)</b>的游戏AI都是使用一个深度神经网络来进行决策，整个框架包括接收游戏环境的观测，利用神经网络获得行为，以及从游戏环境中得到反馈。<br><img src=\"/images/article/Games104/17/Games104_17_71.png\"><span class=\"image-caption\">游戏中的机器学习框架</span></p>\n<h3 id=\"状态（State）\"><a href=\"#状态（State）\" class=\"headerlink\" title=\"状态（State）\"></a>状态（State）</h3><p>以AlphaStar为例，智能体可以直接从游戏环境获得的信息包括地图、统计数据、场景中的单位以及资源数据等。<br><img src=\"/images/article/Games104/17/Games104_17_72.png\"><span class=\"image-caption\">DRL示例-状态</span><br><img src=\"/images/article/Games104/17/Games104_17_73.png\"><span class=\"image-caption\">状态-地图</span><br><img src=\"/images/article/Games104/17/Games104_17_74.png\"><span class=\"image-caption\">状态2-地图</span></p>\n<h3 id=\"行动（Actions）\"><a href=\"#行动（Actions）\" class=\"headerlink\" title=\"行动（Actions）\"></a>行动（Actions）</h3><p>在AlphaStar中智能体的行为还取决于当前选中的单位。<br><img src=\"/images/article/Games104/17/Games104_17_75.png\"><span class=\"image-caption\">行动</span></p>\n<h3 id=\"奖励（Rewards）\"><a href=\"#奖励（Rewards）\" class=\"headerlink\" title=\"奖励（Rewards）\"></a>奖励（Rewards）</h3><p>奖励函数的设计对于模型的训练以及最终的性能都有着重要的影响。在AlphaStar中使用了非常简单的奖励设计，智能体仅在获胜时获得+1的奖励；而在OpenAI Five中则采用了更加复杂的奖励函数并以此来鼓励AI的进攻性。<br><img src=\"/images/article/Games104/17/Games104_17_76.png\"><span class=\"image-caption\">奖励</span><br><img src=\"/images/article/Games104/17/Games104_17_77.png\"><span class=\"image-caption\">奖励2</span></p>\n<h3 id=\"神经网络（Network）\"><a href=\"#神经网络（Network）\" class=\"headerlink\" title=\"神经网络（Network）\"></a>神经网络（Network）</h3><p>在AlphaStar中使用了不同种类的神经网络来处理不同类型的输入数据，比如说对于定长的输入使用了MLP，对于图像数据使用了CNN，对于非定长的序列使用了Transformer，而对于整个决策过程还使用了LSTM进行处理。<br><img src=\"/images/article/Games104/17/Games104_17_78.png\"><span class=\"image-caption\">NN体系结构</span><br><img src=\"/images/article/Games104/17/Games104_17_79.png\"><span class=\"image-caption\">DRL示例——多层感知器（MLP）</span><br><img src=\"/images/article/Games104/17/Games104_17_80.png\"><span class=\"image-caption\">DRL示例-卷积神经网络（CNN）</span><br><img src=\"/images/article/Games104/17/Games104_17_81.png\"><span class=\"image-caption\">DRL示例-转换器</span><br><img src=\"/images/article/Games104/17/Games104_17_82.png\"><span class=\"image-caption\">DRL示例-长短期记忆递归（LSTM）</span><br><img src=\"/images/article/Games104/17/Games104_17_83.png\"><span class=\"image-caption\">DRL示例——神经网络架构的选择</span></p>\n<h3 id=\"训练策略（Training-Strategy）\"><a href=\"#训练策略（Training-Strategy）\" class=\"headerlink\" title=\"训练策略（Training Strategy）\"></a>训练策略（Training Strategy）</h3><p>除此之外，AlphaStar还对模型的训练过程进行了大规模的革新。在AlphaStar的训练过程中首先使用了监督学习的方式来从人类玩家的录像中进行学习。<br><img src=\"/images/article/Games104/17/Games104_17_84.png\"><span class=\"image-caption\">训练策略-监督学习</span></p>\n<p>接着，AlphaStar使用了强化学习的方法来进行自我训练。<br><img src=\"/images/article/Games104/17/Games104_17_85.png\"><span class=\"image-caption\">训练策略-强化学习</span><br><img src=\"/images/article/Games104/17/Games104_17_86.png\"><span class=\"image-caption\">训练代理-自我游戏和对抗性</span></p>\n<p>试验结果分析表明基于监督学习训练的游戏AI其行为会比较接近于人类玩家，但基本无法超过人类玩家的水平；而基于强化学习训练的AI则可能会有超过玩家的游戏水平，不过需要注意的是使用强化学习可能需要非常多的训练资源。<br><img src=\"/images/article/Games104/17/Games104_17_87.png\"><span class=\"image-caption\">RL还是SL？——SL分析</span><br><img src=\"/images/article/Games104/17/Games104_17_88.png\"><span class=\"image-caption\">RL还是SL？——RL分析</span></p>\n<p>因此对于游戏AI到底是使用监督学习还是使用强化学习进行训练需要结合实际的游戏环境进行考虑。对于奖励比较密集的环境可以直接使用强化学习进行训练，而对于奖励比较稀疏的环境则推荐使用监督学习。<br><img src=\"/images/article/Games104/17/Games104_17_89.png\"><span class=\"image-caption\">RL还是SL？——密集的奖励</span><br><img src=\"/images/article/Games104/17/Games104_17_90.png\"><span class=\"image-caption\">RL还是SL？——总结</span></p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p><a href=\"https://peng00bo00.github.io/2022/07/28/GAMES104-NOTES-17.html\">参考文章</a></p>\n<p><a href=\"https://www.bilibili.com/video/BV1iG4y1i78Q/?spm_id_from=333.788&amp;vd_source=371bc0e94a8c97f991c4ac20af0b2d53\">课程视频</a></p>\n<p><a href=\"https://www.bilibili.com/video/BV1ja411U7zK/?spm_id_from=333.788&amp;vd_source=371bc0e94a8c97f991c4ac20af0b2d53\">课程视频2</a></p>\n<p><a href=\"https://cdn.boomingtech.com/games104_static/upload/GAME104_Lecture17_Gameplay%20Systems-Advanced%20Artificial%20Intelligence.pdf\">课件PPT</a></p>\n","more":"<h2 id=\"高级AI大纲\"><a href=\"#高级AI大纲\" class=\"headerlink\" title=\"高级AI大纲\"></a>高级AI大纲</h2><p><img  src=\"/images/article/Games104/17/Games104_17_01.png\"  ><span class=\"image-caption\">高级AI大纲</span></p>\n<h2 id=\"层次任务网络（Hierarchical-Tasks-Network）\"><a href=\"#层次任务网络（Hierarchical-Tasks-Network）\" class=\"headerlink\" title=\"层次任务网络（Hierarchical Tasks Network）\"></a>层次任务网络（Hierarchical Tasks Network）</h2><p><b>层次任务网络(hierarchical tasks network, HTN)</b>是经典的游戏AI技术，和上一节介绍过的行为树相比HTN可以更好地表达AI自身的意志和驱动力。<br><img  src=\"/images/article/Games104/17/Games104_17_02.png\"  ><span class=\"image-caption\">概括</span></p>\n<p>HTN的思想是把总体目标分解成若干个步骤，其中每个步骤可以包含不同的选项。AI在执行时需要按照顺序完成每个步骤，并且根据自身的状态选择合适的行为。<br><img  src=\"/images/article/Games104/17/Games104_17_03.png\"  ><span class=\"image-caption\">像人一样制定一个计划</span></p>\n<h3 id=\"分层任务网络框架（HTN-Framework）\"><a href=\"#分层任务网络框架（HTN-Framework）\" class=\"headerlink\" title=\"分层任务网络框架（HTN Framework）\"></a>分层任务网络框架（HTN Framework）</h3><p>HTN框架中包含两部分，<b>world state</b>和<b>sensor</b>两部分。其中world state是AI对于游戏世界的认知，而sensor则是AI从游戏世界获取信息的渠道。<br><img  src=\"/images/article/Games104/17/Games104_17_04.png\"  ><span class=\"image-caption\">分层任务网络框架</span></p>\n<p>除此之外HTN还包括<b>domain</b>，<b>planner</b>以及<b>plan runner</b>来表示AI的规划以及执行规划的过程。<br><img  src=\"/images/article/Games104/17/Games104_17_05.png\"  ><span class=\"image-caption\">分层任务网络框架2</span></p>\n<h3 id=\"分层网络任务类型（HTN-Task-Types）\"><a href=\"#分层网络任务类型（HTN-Task-Types）\" class=\"headerlink\" title=\"分层网络任务类型（HTN Task Types）\"></a>分层网络任务类型（HTN Task Types）</h3><p>在HTN中我们将任务分为两类，<b>primitive task</b>和<b>compound task</b>。<br><img  src=\"/images/article/Games104/17/Games104_17_06.png\"  ><span class=\"image-caption\">分层网络任务类型</span></p>\n<p>primitive task一般表示一个具体的动作或行为。在HTN中每个primitive task需要包含precondition、action以及effects三个要素。<br><img  src=\"/images/article/Games104/17/Games104_17_07.png\"  ><span class=\"image-caption\">原始任务</span><br><img  src=\"/images/article/Games104/17/Games104_17_08.png\"  ><span class=\"image-caption\">原始任务2</span></p>\n<p>而compound task则包含不同的方法，我们把这些方法按照一定的优先级组织起来并且在执行时按照优先级高到低的顺序进行选择。每个方法还可以包含其它的primitive task或者compound task，当方法内所有的task都执行完毕则表示任务完成。<br><img  src=\"/images/article/Games104/17/Games104_17_09.png\"  ><span class=\"image-caption\">复合任务</span><br><img  src=\"/images/article/Games104/17/Games104_17_10.png\"  ><span class=\"image-caption\">复合任务2</span></p>\n<p>在此基础上就可以构造出整个HTN的domain，从而实现AI的行为逻辑。<br><img  src=\"/images/article/Games104/17/Games104_17_11.png\"  ><span class=\"image-caption\">HTN域</span><br><img  src=\"/images/article/Games104/17/Games104_17_12.png\"  ><span class=\"image-caption\">HTN域2</span></p>\n<h3 id=\"规划（Planning）\"><a href=\"#规划（Planning）\" class=\"headerlink\" title=\"规划（Planning）\"></a>规划（Planning）</h3><p>接下来就可以进行规划了，我们从root task出发不断进行展开逐步完成每个任务。<br><img  src=\"/images/article/Games104/17/Games104_17_13.png\"  ><span class=\"image-caption\">规划</span><br><img  src=\"/images/article/Games104/17/Games104_17_14.png\"  ><span class=\"image-caption\">规划2</span><br><img  src=\"/images/article/Games104/17/Games104_17_15.png\"  ><span class=\"image-caption\">规划3</span><br><img  src=\"/images/article/Games104/17/Games104_17_16.png\"  ><span class=\"image-caption\">规划4</span><br><img  src=\"/images/article/Games104/17/Games104_17_17.png\"  ><span class=\"image-caption\">规划5</span><br><img  src=\"/images/article/Games104/17/Games104_17_18.png\"  ><span class=\"image-caption\">规划6</span><br><img  src=\"/images/article/Games104/17/Games104_17_19.png\"  ><span class=\"image-caption\">规划7</span></p>\n<h3 id=\"重新规划（Replan）\"><a href=\"#重新规划（Replan）\" class=\"headerlink\" title=\"重新规划（Replan）\"></a>重新规划（Replan）</h3><p>执行plan时需要注意有时任务会失败，这就需要我们重新进行规划，这一过程称为<b>replan</b>。<br><img  src=\"/images/article/Games104/17/Games104_17_20.png\"  ><span class=\"image-caption\">运行规划</span></p>\n<p>当plan执行完毕或是发生失败，亦或是world state发生改变后就需要进行replan。<br><img  src=\"/images/article/Games104/17/Games104_17_21.png\"  ><span class=\"image-caption\">重新规划</span></p>\n<p>总结一下HTN和BT非常相似，但它更加符合人的直觉也更易于设计师进行掌握。<br><img  src=\"/images/article/Games104/17/Games104_17_22.png\"  ><span class=\"image-caption\">总结</span></p>\n<h2 id=\"目标导向的AI系统（Goal-Oriented-Action-Planning-GOAP）\"><a href=\"#目标导向的AI系统（Goal-Oriented-Action-Planning-GOAP）\" class=\"headerlink\" title=\"目标导向的AI系统（Goal-Oriented Action Planning - GOAP）\"></a>目标导向的AI系统（Goal-Oriented Action Planning - GOAP）</h2><p><b>goal-oriented action planning(GOAP)</b>是一种基于规划的AI技术，和前面介绍过的方法相比GOAP一般会更适合动态的环境。<br><img  src=\"/images/article/Games104/17/Games104_17_23.png\"  ><span class=\"image-caption\">目标导向的AI系统</span></p>\n<h3 id=\"结构（Structure）\"><a href=\"#结构（Structure）\" class=\"headerlink\" title=\"结构（Structure）\"></a>结构（Structure）</h3><p>GOAP的整体结构与HTN非常相似，不过在GOAP中domain被替换为<b>goal set</b>和<b>action set</b>。<br><img  src=\"/images/article/Games104/17/Games104_17_24.png\"  ><span class=\"image-caption\">结构</span></p>\n<p>goal set表示AI所有可以达成的目标。在GOAP中需要显式地定义可以实现的目标，这要求我们把目标使用相应的状态来进行表达。<br><img  src=\"/images/article/Games104/17/Games104_17_25.png\"  ><span class=\"image-caption\">目标集合</span><br><img  src=\"/images/article/Games104/17/Games104_17_26.png\"  ><span class=\"image-caption\">目标选择</span></p>\n<p>而action set则接近于primitive task的概念，它表示AI可以执行的行为。需要注意的是action set还包含<b>代价(cost)</b>的概念，它表示不同动作的”优劣”程度。在进行规划时我们希望AI尽可能做出代价小的决策。<br><img  src=\"/images/article/Games104/17/Games104_17_27.png\"  ><span class=\"image-caption\">操作集</span></p>\n<h3 id=\"规划（Planning）-1\"><a href=\"#规划（Planning）-1\" class=\"headerlink\" title=\"规划（Planning）\"></a>规划（Planning）</h3><p>GOAP在进行规划时会从目标来倒推需要执行的动作，这一过程称为<b>反向规划(backward planning)</b>。<br><img  src=\"/images/article/Games104/17/Games104_17_28.png\"  ><span class=\"image-caption\">像一个人一样反向规划</span></p>\n<p>在进行规划时首先需要根据优先级来选取一个目标，然后查询实现目标需要满足的状态。为了满足这些状态需求，我们需要从action set中选择一系列动作。需要注意的是很多动作也有自身的状态需求，因此我们在选择动作时也需要把这些需求添加到列表中。最后不断地添加动作和需求直到所有的状态需求都得到了满足，这样就完成了反向规划。<br><img  src=\"/images/article/Games104/17/Games104_17_29.png\"  ><span class=\"image-caption\">规划</span><br><img  src=\"/images/article/Games104/17/Games104_17_30.png\"  ><span class=\"image-caption\">规划2</span><br><img  src=\"/images/article/Games104/17/Games104_17_31.png\"  ><span class=\"image-caption\">规划3</span><br><img  src=\"/images/article/Games104/17/Games104_17_32.png\"  ><span class=\"image-caption\">规划4</span></p>\n<p>GOAP的难点在于如何从action set进行选择，我们要求状态需求都能够得到满足而且所添加动作的代价要尽可能小。显然这样的问题是一个<b>动态规划(dynamic programming)</b>问题，我们可以利用图这样的数据结构来进行求解。在构造图时把状态的组合作为图上的节点，不同节点之间的有向边表示可以执行的动作，边的权重则是动作的代价。这样整个规划问题就等价于在有向图上的最短路径问题。<br><img  src=\"/images/article/Games104/17/Games104_17_33.png\"  ><span class=\"image-caption\">构建状态-行动成本图</span><br><img  src=\"/images/article/Games104/17/Games104_17_34.png\"  ><span class=\"image-caption\">最低成本路径</span></p>\n<p>总结一下GOAP可以让AI的行为更加动态，而且可以有效地解耦AI的目标与行为；而GOAP的主要缺陷在于它会比较消耗计算资源，一般情况下GOAP需要的计算量会远高于BT和HTN。<br><img  src=\"/images/article/Games104/17/Games104_17_35.png\"  ><span class=\"image-caption\">总结</span></p>\n<h2 id=\"蒙特卡洛树搜索（Monte-Carlo-Tree-Search）\"><a href=\"#蒙特卡洛树搜索（Monte-Carlo-Tree-Search）\" class=\"headerlink\" title=\"蒙特卡洛树搜索（Monte Carlo Tree Search）\"></a>蒙特卡洛树搜索（Monte Carlo Tree Search）</h2><p><b>蒙特卡洛树搜索(Monte Carlo tree search, MCTS)</b>也是经典的AI算法，实际上AlphaGo就是基于MCTS来实现的。简单来说，MCTS的思路是在进行决策时首先模拟大量可行的动作，然后从这些动作中选择最好的那个来执行。<br><img  src=\"/images/article/Games104/17/Games104_17_36.png\"  ><span class=\"image-caption\">蒙特卡洛树搜索</span><br><img  src=\"/images/article/Games104/17/Games104_17_37.png\"  ><span class=\"image-caption\">蒙特卡洛树搜索2</span></p>\n<p>MCTS的核心是<b>Monte Carlo方法(Monte Carlo method)</b>，它指出定积分可以通过随机采样的方法来进行估计。<br><img  src=\"/images/article/Games104/17/Games104_17_38.png\"  ><span class=\"image-caption\">Monte Carlo方法</span></p>\n<p>以围棋为例，MCTS会根据当前棋盘上的状态来估计落子的位置。<br><img  src=\"/images/article/Games104/17/Games104_17_39.png\"  ><span class=\"image-caption\">蒙特卡洛树搜索3</span></p>\n<p>从数学的角度来看，我们把棋盘上棋子的位置称为<b>状态(state)</b>，同时把落子的过程称为<b>行为(action)</b>。这样整个游戏可以建模为从初始节点出发的状态转移过程，而且所有可能的状态转移可以表示为一棵树。<br><img  src=\"/images/article/Games104/17/Games104_17_40.png\"  ><span class=\"image-caption\">状态和行动</span><br><img  src=\"/images/article/Games104/17/Games104_17_41.png\"  ><span class=\"image-caption\">状态转移</span><br><img  src=\"/images/article/Games104/17/Games104_17_42.png\"  ><span class=\"image-caption\">状态空间</span></p>\n<p>显然构造出完整的树结构可能是非常困难的，不过实际上我们并不需要完整的树。在使用MCTS时，完成每一个行为后只需要重新以当前状态构造一棵新树即可。<br><img  src=\"/images/article/Games104/17/Games104_17_43.png\"  ><span class=\"image-caption\">注意：重建每次移动重建状态空间</span></p>\n<h3 id=\"模拟（Simulation）\"><a href=\"#模拟（Simulation）\" class=\"headerlink\" title=\"模拟（Simulation）\"></a>模拟（Simulation）</h3><p><b>模拟(simulation)</b>是MCTS中的重要一环，这里的”模拟”是指AI利用当前的策略快速地完成整个游戏过程。<br><img  src=\"/images/article/Games104/17/Games104_17_44.png\"  ><span class=\"image-caption\">模拟：快速地玩游戏</span></p>\n<h3 id=\"反向传播（Backpropagate）\"><a href=\"#反向传播（Backpropagate）\" class=\"headerlink\" title=\"反向传播（Backpropagate）\"></a>反向传播（Backpropagate）</h3><p>我们从同一节点出发进行不断的模拟就可以估计该节点的价值(胜率)。<br><img  src=\"/images/article/Games104/17/Games104_17_45.png\"  ><span class=\"image-caption\">如何评估这些状态？</span></p>\n<p>然后把模拟的结果从下向上进行传播就可以更新整个决策序列上所有节点的价值。<br><img  src=\"/images/article/Games104/17/Games104_17_46.png\"  ><span class=\"image-caption\">反向传播</span></p>\n<h3 id=\"迭代步骤（Iteration-Steps）\"><a href=\"#迭代步骤（Iteration-Steps）\" class=\"headerlink\" title=\"迭代步骤（Iteration Steps）\"></a>迭代步骤（Iteration Steps）</h3><p>这样我们就可以定义MCTS的迭代步骤如下：<br><img  src=\"/images/article/Games104/17/Games104_17_47.png\"  ><span class=\"image-caption\">迭代步骤</span><br><img  src=\"/images/article/Games104/17/Games104_17_48.png\"  ><span class=\"image-caption\">在“无限”状态空间中进行搜索</span></p>\n<h4 id=\"选择（Selection）\"><a href=\"#选择（Selection）\" class=\"headerlink\" title=\"选择（Selection）\"></a>选择（Selection）</h4><p>在对节点进行选择时，MCTS会优先选择可拓展的节点。<br><img  src=\"/images/article/Games104/17/Games104_17_49.png\"  ><span class=\"image-caption\">选择-可扩展节点</span></p>\n<p>在进行拓展时往往还要权衡一些exploitation和exploration，因此我们可以把UCB可以作为一种拓展的准则。<br><img  src=\"/images/article/Games104/17/Games104_17_50.png\"  ><span class=\"image-caption\">选择-开发与勘探</span><br><img  src=\"/images/article/Games104/17/Games104_17_51.png\"  ><span class=\"image-caption\">UCB（上置信度区间）</span></p>\n<p>这样在进行选择时首先需要从根节点出发然后不断选择当前UCB最大的那个节点向下进行访问，当访问到一个没有拓展过的节点时选择该节点进行展开。<br><img  src=\"/images/article/Games104/17/Games104_17_52.png\"  ><span class=\"image-caption\">选择</span></p>\n<h4 id=\"拓展（Expansion）\"><a href=\"#拓展（Expansion）\" class=\"headerlink\" title=\"拓展（Expansion）\"></a>拓展（Expansion）</h4><p>对节点进行展开时我们需要根据可执行的动作选择一组进行模拟，然后把模拟的结果自下而上进行传播。<br><img  src=\"/images/article/Games104/17/Games104_17_53.png\"  ><span class=\"image-caption\">拓展</span><br><img  src=\"/images/article/Games104/17/Games104_17_54.png\"  ><span class=\"image-caption\">模拟和回溯</span></p>\n<h4 id=\"最终条件（The-End-Condition）\"><a href=\"#最终条件（The-End-Condition）\" class=\"headerlink\" title=\"最终条件（The End Condition）\"></a>最终条件（The End Condition）</h4><p>当对树的探索达到一定程度后就可以终止拓展过程，此时我们就得到了树结构上每个节点的价值。<br><img  src=\"/images/article/Games104/17/Games104_17_55.png\"  ><span class=\"image-caption\">最终条件</span></p>\n<p>然后只需要回到根节点选择一个最优的子节点进行执行即可。<br><img  src=\"/images/article/Games104/17/Games104_17_56.png\"  ><span class=\"image-caption\">如何选择最好的移动？</span></p>\n<p>总结一下，MCTS是一种非常强大的决策算法而且很适合搜索空间巨大的决策问题；而它的主要缺陷在于它具有过大的计算复杂度，而且它的效果很大程度上依赖于状态和行为空间的设计。<br><img  src=\"/images/article/Games104/17/Games104_17_57.png\"  ><span class=\"image-caption\">总结</span></p>\n<h2 id=\"机器学习基础（Machine-Learning-Basic）\"><a href=\"#机器学习基础（Machine-Learning-Basic）\" class=\"headerlink\" title=\"机器学习基础（Machine Learning Basic）\"></a>机器学习基础（Machine Learning Basic）</h2><h3 id=\"机器学习技术（ML-Types）\"><a href=\"#机器学习技术（ML-Types）\" class=\"headerlink\" title=\"机器学习技术（ML Types）\"></a>机器学习技术（ML Types）</h3><p>近几年在<b>机器学习(machine learning, ML)</b>技术的不断发展下有越来越多的游戏AI开始使用机器学习来进行实现。根据学习的方式，机器学习大致可以分为监督学习、无监督学习、半监督学习以及强化学习等几类。<br><img  src=\"/images/article/Games104/17/Games104_17_58.png\"  ><span class=\"image-caption\">四种类型的机器学习</span><br><img  src=\"/images/article/Games104/17/Games104_17_59.png\"  ><span class=\"image-caption\">监督学习</span><br><img  src=\"/images/article/Games104/17/Games104_17_60.png\"  ><span class=\"image-caption\">无监督学习</span><br><img  src=\"/images/article/Games104/17/Games104_17_61.png\"  ><span class=\"image-caption\">半监督学习</span></p>\n<p><b>强化学习(reinforcement learning, RL)</b>是游戏AI技术的基础。在强化学习中我们希望AI能够通过和环境的不断互动来学习到一个合理的策略。<br><img  src=\"/images/article/Games104/17/Games104_17_62.png\"  ><span class=\"image-caption\">强化学习</span><br><img  src=\"/images/article/Games104/17/Games104_17_63.png\"  ><span class=\"image-caption\">强化学习2</span></p>\n<h3 id=\"Markov决策过程（Markov-Decision-Process）\"><a href=\"#Markov决策过程（Markov-Decision-Process）\" class=\"headerlink\" title=\"Markov决策过程（Markov Decision Process）\"></a>Markov决策过程（Markov Decision Process）</h3><p>强化学习的理论基础是<b>Markov决策过程(Markov decision process, MDP)</b>。在MDP中智能体对环境的感知称为<b>状态(state)</b>，环境对于智能体的反馈称为<b>奖励(reward)</b>。MDP的目标是让智能体通过和环境不断的互动来学习到如何在不同的环境下进行决策，这样的一个决策函数称为<b>策略(policy)</b>。<br><img  src=\"/images/article/Games104/17/Games104_17_64.png\"  ><span class=\"image-caption\">Markov决策过程-基本要素</span><br><img  src=\"/images/article/Games104/17/Games104_17_65.png\"  ><span class=\"image-caption\">Markov决策过程-状态</span><br><img  src=\"/images/article/Games104/17/Games104_17_66.png\"  ><span class=\"image-caption\">Markov决策过程-行动</span><br><img  src=\"/images/article/Games104/17/Games104_17_67.png\"  ><span class=\"image-caption\">Markov决策过程-奖励</span><br><img  src=\"/images/article/Games104/17/Games104_17_68.png\"  ><span class=\"image-caption\">MDP数学模型</span><br><img  src=\"/images/article/Games104/17/Games104_17_69.png\"  ><span class=\"image-caption\">策略</span></p>\n<h2 id=\"构建高级游戏AI（Build-Advanced-Game-AI）\"><a href=\"#构建高级游戏AI（Build-Advanced-Game-AI）\" class=\"headerlink\" title=\"构建高级游戏AI（Build Advanced Game AI）\"></a>构建高级游戏AI（Build Advanced Game AI）</h2><p>尽管目前基于机器学习的游戏AI技术大多还处于试验阶段，但已经有一些很优秀的项目值得借鉴和学习，包括DeepMind的AlphaStar以及OpenAI的Five等。<br><img  src=\"/images/article/Games104/17/Games104_17_70.png\"  ><span class=\"image-caption\">为什么游戏人工智能需要机器学习</span></p>\n<p>这些基于<b>深度强化学习(deep reinforcement learning, DRL)</b>的游戏AI都是使用一个深度神经网络来进行决策，整个框架包括接收游戏环境的观测，利用神经网络获得行为，以及从游戏环境中得到反馈。<br><img  src=\"/images/article/Games104/17/Games104_17_71.png\"  ><span class=\"image-caption\">游戏中的机器学习框架</span></p>\n<h3 id=\"状态（State）\"><a href=\"#状态（State）\" class=\"headerlink\" title=\"状态（State）\"></a>状态（State）</h3><p>以AlphaStar为例，智能体可以直接从游戏环境获得的信息包括地图、统计数据、场景中的单位以及资源数据等。<br><img  src=\"/images/article/Games104/17/Games104_17_72.png\"  ><span class=\"image-caption\">DRL示例-状态</span><br><img  src=\"/images/article/Games104/17/Games104_17_73.png\"  ><span class=\"image-caption\">状态-地图</span><br><img  src=\"/images/article/Games104/17/Games104_17_74.png\"  ><span class=\"image-caption\">状态2-地图</span></p>\n<h3 id=\"行动（Actions）\"><a href=\"#行动（Actions）\" class=\"headerlink\" title=\"行动（Actions）\"></a>行动（Actions）</h3><p>在AlphaStar中智能体的行为还取决于当前选中的单位。<br><img  src=\"/images/article/Games104/17/Games104_17_75.png\"  ><span class=\"image-caption\">行动</span></p>\n<h3 id=\"奖励（Rewards）\"><a href=\"#奖励（Rewards）\" class=\"headerlink\" title=\"奖励（Rewards）\"></a>奖励（Rewards）</h3><p>奖励函数的设计对于模型的训练以及最终的性能都有着重要的影响。在AlphaStar中使用了非常简单的奖励设计，智能体仅在获胜时获得+1的奖励；而在OpenAI Five中则采用了更加复杂的奖励函数并以此来鼓励AI的进攻性。<br><img  src=\"/images/article/Games104/17/Games104_17_76.png\"  ><span class=\"image-caption\">奖励</span><br><img  src=\"/images/article/Games104/17/Games104_17_77.png\"  ><span class=\"image-caption\">奖励2</span></p>\n<h3 id=\"神经网络（Network）\"><a href=\"#神经网络（Network）\" class=\"headerlink\" title=\"神经网络（Network）\"></a>神经网络（Network）</h3><p>在AlphaStar中使用了不同种类的神经网络来处理不同类型的输入数据，比如说对于定长的输入使用了MLP，对于图像数据使用了CNN，对于非定长的序列使用了Transformer，而对于整个决策过程还使用了LSTM进行处理。<br><img  src=\"/images/article/Games104/17/Games104_17_78.png\"  ><span class=\"image-caption\">NN体系结构</span><br><img  src=\"/images/article/Games104/17/Games104_17_79.png\"  ><span class=\"image-caption\">DRL示例——多层感知器（MLP）</span><br><img  src=\"/images/article/Games104/17/Games104_17_80.png\"  ><span class=\"image-caption\">DRL示例-卷积神经网络（CNN）</span><br><img  src=\"/images/article/Games104/17/Games104_17_81.png\"  ><span class=\"image-caption\">DRL示例-转换器</span><br><img  src=\"/images/article/Games104/17/Games104_17_82.png\"  ><span class=\"image-caption\">DRL示例-长短期记忆递归（LSTM）</span><br><img  src=\"/images/article/Games104/17/Games104_17_83.png\"  ><span class=\"image-caption\">DRL示例——神经网络架构的选择</span></p>\n<h3 id=\"训练策略（Training-Strategy）\"><a href=\"#训练策略（Training-Strategy）\" class=\"headerlink\" title=\"训练策略（Training Strategy）\"></a>训练策略（Training Strategy）</h3><p>除此之外，AlphaStar还对模型的训练过程进行了大规模的革新。在AlphaStar的训练过程中首先使用了监督学习的方式来从人类玩家的录像中进行学习。<br><img  src=\"/images/article/Games104/17/Games104_17_84.png\"  ><span class=\"image-caption\">训练策略-监督学习</span></p>\n<p>接着，AlphaStar使用了强化学习的方法来进行自我训练。<br><img  src=\"/images/article/Games104/17/Games104_17_85.png\"  ><span class=\"image-caption\">训练策略-强化学习</span><br><img  src=\"/images/article/Games104/17/Games104_17_86.png\"  ><span class=\"image-caption\">训练代理-自我游戏和对抗性</span></p>\n<p>试验结果分析表明基于监督学习训练的游戏AI其行为会比较接近于人类玩家，但基本无法超过人类玩家的水平；而基于强化学习训练的AI则可能会有超过玩家的游戏水平，不过需要注意的是使用强化学习可能需要非常多的训练资源。<br><img  src=\"/images/article/Games104/17/Games104_17_87.png\"  ><span class=\"image-caption\">RL还是SL？——SL分析</span><br><img  src=\"/images/article/Games104/17/Games104_17_88.png\"  ><span class=\"image-caption\">RL还是SL？——RL分析</span></p>\n<p>因此对于游戏AI到底是使用监督学习还是使用强化学习进行训练需要结合实际的游戏环境进行考虑。对于奖励比较密集的环境可以直接使用强化学习进行训练，而对于奖励比较稀疏的环境则推荐使用监督学习。<br><img  src=\"/images/article/Games104/17/Games104_17_89.png\"  ><span class=\"image-caption\">RL还是SL？——密集的奖励</span><br><img  src=\"/images/article/Games104/17/Games104_17_90.png\"  ><span class=\"image-caption\">RL还是SL？——总结</span></p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p><a href=\"https://peng00bo00.github.io/2022/07/28/GAMES104-NOTES-17.html\">参考文章</a></p>\n<p><a href=\"https://www.bilibili.com/video/BV1iG4y1i78Q/?spm_id_from=333.788&vd_source=371bc0e94a8c97f991c4ac20af0b2d53\">课程视频</a></p>\n<p><a href=\"https://www.bilibili.com/video/BV1ja411U7zK/?spm_id_from=333.788&vd_source=371bc0e94a8c97f991c4ac20af0b2d53\">课程视频2</a></p>\n<p><a href=\"https://cdn.boomingtech.com/games104_static/upload/GAME104_Lecture17_Gameplay%20Systems-Advanced%20Artificial%20Intelligence.pdf\">课件PPT</a></p>\n","categories":[{"name":"游戏引擎","path":"api/categories/游戏引擎.json"}],"tags":[{"name":"游戏引擎","path":"api/tags/游戏引擎.json"},{"name":"games104","path":"api/tags/games104.json"}]}